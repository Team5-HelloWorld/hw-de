{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tent review 데이터 클렌징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import mean, col, split, regexp_extract, when, lit\n",
    "# ML\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark 설정\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"uber-date-trips\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스파크 세션 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ec2-user/notebook-work/miran/cleansing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('tent-review-cleansing')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(os.getcwd() + '/tent-review.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>prd_id</th>\n",
       "      <th>prd_url</th>\n",
       "      <th>user_id</th>\n",
       "      <th>review</th>\n",
       "      <th>uploaded_date</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>star_score</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14981947918</td>\n",
       "      <td>https://smartstore.naver.com/main/products/307...</td>\n",
       "      <td>nana****</td>\n",
       "      <td>원래 원터치텐트만 구입하려고 마음먹은 사람입니다. 기존 패스트캠프 테라6 원터치 텐...</td>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>0.894785</td>\n",
       "      <td>5</td>\n",
       "      <td>['https://phinf.pstatic.net/checkout/20220502_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14981947918</td>\n",
       "      <td>https://smartstore.naver.com/main/products/307...</td>\n",
       "      <td>pyun****</td>\n",
       "      <td>좋아요.한여름에 창이 4개가 아니라 고생했지많요..&lt;br&gt;확실히 두개라서 바람이 잘...</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>0.892367</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14981947918</td>\n",
       "      <td>https://smartstore.naver.com/main/products/307...</td>\n",
       "      <td>eori****</td>\n",
       "      <td>1.배송이 빨리왔어요^^&lt;br&gt;2.펼치고 접기 너무 쉬워요~&lt;br&gt;  ㅡ사실 저희부...</td>\n",
       "      <td>2021-08-04</td>\n",
       "      <td>0.88553</td>\n",
       "      <td>5</td>\n",
       "      <td>['https://phinf.pstatic.net/checkout/20210804_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>14981947918</td>\n",
       "      <td>https://smartstore.naver.com/main/products/307...</td>\n",
       "      <td>ider****</td>\n",
       "      <td>구매하기전에 정말 고민 많이했어요~~ &lt;br&gt;아이와 캠핑가기 위해 설치가 편한 제품...</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>0.880135</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14981947918</td>\n",
       "      <td>https://smartstore.naver.com/main/products/307...</td>\n",
       "      <td>rudc****</td>\n",
       "      <td>첫 캠핑이라 너무 설레서 &lt;em&gt;고민고민하다가 무엇을 사야하나 하고 &lt;/em&gt;매장도...</td>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>0.880015</td>\n",
       "      <td>5</td>\n",
       "      <td>['https://phinf.pstatic.net/checkout/20210902_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181343</th>\n",
       "      <td>107236</td>\n",
       "      <td>2471807188</td>\n",
       "      <td>https://smartstore.naver.com/main/products/247...</td>\n",
       "      <td>1you******</td>\n",
       "      <td>가격대비 가성비 좋아요</td>\n",
       "      <td>2019-09-11 13:59:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181344</th>\n",
       "      <td>107237</td>\n",
       "      <td>2471807188</td>\n",
       "      <td>https://smartstore.naver.com/main/products/247...</td>\n",
       "      <td>hong*****</td>\n",
       "      <td>상품좋습니다 크기도생각보다큼</td>\n",
       "      <td>2019-09-08 00:46:23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181345</th>\n",
       "      <td>107238</td>\n",
       "      <td>2471807188</td>\n",
       "      <td>https://smartstore.naver.com/main/products/247...</td>\n",
       "      <td>kooh******</td>\n",
       "      <td>색깔이 흰색인줄 알았는데 회색이더군요</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181346</th>\n",
       "      <td>아직 펴보진 않았는데 일단 가볍긴하네요\"</td>\n",
       "      <td>2019-09-01 23:33:03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181347</th>\n",
       "      <td>107239</td>\n",
       "      <td>2471807188</td>\n",
       "      <td>https://smartstore.naver.com/main/products/247...</td>\n",
       "      <td>wpxk***</td>\n",
       "      <td>만족합니다만족핮니더</td>\n",
       "      <td>2019-08-31 08:35:49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181348 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _c0               prd_id  \\\n",
       "0                            0          14981947918   \n",
       "1                            1          14981947918   \n",
       "2                            2          14981947918   \n",
       "3                            3          14981947918   \n",
       "4                            4          14981947918   \n",
       "...                        ...                  ...   \n",
       "181343                  107236           2471807188   \n",
       "181344                  107237           2471807188   \n",
       "181345                  107238           2471807188   \n",
       "181346  아직 펴보진 않았는데 일단 가볍긴하네요\"  2019-09-01 23:33:03   \n",
       "181347                  107239           2471807188   \n",
       "\n",
       "                                                  prd_url     user_id  \\\n",
       "0       https://smartstore.naver.com/main/products/307...    nana****   \n",
       "1       https://smartstore.naver.com/main/products/307...    pyun****   \n",
       "2       https://smartstore.naver.com/main/products/307...    eori****   \n",
       "3       https://smartstore.naver.com/main/products/307...    ider****   \n",
       "4       https://smartstore.naver.com/main/products/307...    rudc****   \n",
       "...                                                   ...         ...   \n",
       "181343  https://smartstore.naver.com/main/products/247...  1you******   \n",
       "181344  https://smartstore.naver.com/main/products/247...   hong*****   \n",
       "181345  https://smartstore.naver.com/main/products/247...  kooh******   \n",
       "181346                                                0.0           4   \n",
       "181347  https://smartstore.naver.com/main/products/247...     wpxk***   \n",
       "\n",
       "                                                   review  \\\n",
       "0       원래 원터치텐트만 구입하려고 마음먹은 사람입니다. 기존 패스트캠프 테라6 원터치 텐...   \n",
       "1       좋아요.한여름에 창이 4개가 아니라 고생했지많요..<br>확실히 두개라서 바람이 잘...   \n",
       "2       1.배송이 빨리왔어요^^<br>2.펼치고 접기 너무 쉬워요~<br>  ㅡ사실 저희부...   \n",
       "3       구매하기전에 정말 고민 많이했어요~~ <br>아이와 캠핑가기 위해 설치가 편한 제품...   \n",
       "4       첫 캠핑이라 너무 설레서 <em>고민고민하다가 무엇을 사야하나 하고 </em>매장도...   \n",
       "...                                                   ...   \n",
       "181343                                       가격대비 가성비 좋아요   \n",
       "181344                                    상품좋습니다 크기도생각보다큼   \n",
       "181345                               색깔이 흰색인줄 알았는데 회색이더군요   \n",
       "181346                                                NaN   \n",
       "181347                                         만족합니다만족핮니더   \n",
       "\n",
       "              uploaded_date quality_score star_score  \\\n",
       "0                2022-05-02      0.894785          5   \n",
       "1                2019-10-07      0.892367          3   \n",
       "2                2021-08-04       0.88553          5   \n",
       "3                2022-05-24      0.880135          5   \n",
       "4                2021-09-02      0.880015          5   \n",
       "...                     ...           ...        ...   \n",
       "181343  2019-09-11 13:59:02           0.0          4   \n",
       "181344  2019-09-08 00:46:23           0.0          5   \n",
       "181345                 None          None       None   \n",
       "181346                 None          None       None   \n",
       "181347  2019-08-31 08:35:49           0.0          5   \n",
       "\n",
       "                                                image_url  \n",
       "0       ['https://phinf.pstatic.net/checkout/20220502_...  \n",
       "1                                                     NaN  \n",
       "2       ['https://phinf.pstatic.net/checkout/20210804_...  \n",
       "3                                                     NaN  \n",
       "4       ['https://phinf.pstatic.net/checkout/20210902_...  \n",
       "...                                                   ...  \n",
       "181343                                                NaN  \n",
       "181344                                                NaN  \n",
       "181345                                               None  \n",
       "181346                                               None  \n",
       "181347                                                NaN  \n",
       "\n",
       "[181348 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------+----------------------------------+-------------+-------------+----------+--------------------+\n",
      "|_c0|     prd_id|             prd_url| user_id|                            review|uploaded_date|quality_score|star_score|           image_url|\n",
      "+---+-----------+--------------------+--------+----------------------------------+-------------+-------------+----------+--------------------+\n",
      "|  0|14981947918|https://smartstor...|nana****|원래 원터치텐트만 구입하려고 마...|   2022-05-02|     0.894785|         5|['https://phinf.p...|\n",
      "|  1|14981947918|https://smartstor...|pyun****|  좋아요.한여름에 창이 4개가 아...|   2019-10-07|     0.892367|         3|                 NaN|\n",
      "|  2|14981947918|https://smartstor...|eori****|      1.배송이 빨리왔어요^^<br>...|   2021-08-04|      0.88553|         5|['https://phinf.p...|\n",
      "|  3|14981947918|https://smartstor...|ider****|구매하기전에 정말 고민 많이했어...|   2022-05-24|     0.880135|         5|                 NaN|\n",
      "|  4|14981947918|https://smartstor...|rudc****|    첫 캠핑이라 너무 설레서 <em...|   2021-09-02|     0.880015|         5|['https://phinf.p...|\n",
      "|  5|14981947918|https://smartstor...|unin****|  캠핑 한번도 한적 없어요 설치 ...|   2021-08-29|     0.877089|         5|['https://phinf.p...|\n",
      "|  6|14981947918|https://smartstor...|uhih****|  추석 이후 올줄 알았는데 완전 ...|   2020-09-30|     0.864284|         5|['https://phinf.p...|\n",
      "|  7|14981947918|https://front.wem...|est*****|이걸로 얼마전 태풍같이 바람불고...|   2019-06-27|     0.858249|         5|                 NaN|\n",
      "|  8|14981947918|https://smartstor...|kid1****| 같은날 주문한 품목들중 일부 품...|   2022-06-02|     0.856662|         1|['https://phinf.p...|\n",
      "|  9|14981947918|https://smartstor...|heys****| 택배는 중간에 광복절 휴무가 끼...|   2021-08-18|     0.851376|         4|['https://phinf.p...|\n",
      "| 10|14981947918|https://smartstor...|manb****|         <em>너무 좋네요</em>~ ...|   2021-06-08|     0.832346|         5|['https://phinf.p...|\n",
      "| 11|14981947918|https://smartstor...|keno****|  여름에 잘 썼습니다. 그런데 육...|   2021-08-27|     0.831201|         5|['https://phinf.p...|\n",
      "| 12|14981947918|https://smartstor...|kys0****| 제가 사용 해보니 간편하고 튼튼...|   2021-09-19|     0.828465|         5|['https://phinf.p...|\n",
      "| 13|14981947918|https://smartstor...|appl****|   일단 텐트가 아이둘 있는 4인 ...|   2019-07-24|     0.820368|         4|['https://phinf.p...|\n",
      "| 14|14981947918|https://smartstor...|yyyu****|여름이라 편하게 다니려고 원터치...|   2021-07-11|     0.817555|         5|['https://phinf.p...|\n",
      "| 15|14981947918|https://shopping....|dksw****|간단하게 후기를 남기자면 텐트가...|   2020-04-26|      0.81507|         3|['https://phinf.p...|\n",
      "| 16|14981947918|                 NaN|ji******|    아이들도 크고 <em>날씨도 좋...|   2020-03-29|     0.808382|         5|                 NaN|\n",
      "| 17|14981947918|https://smartstor...|rlav****|남편이 체력거지 게으름뱅이 귀차...|   2021-06-23|     0.803624|         5|['https://phinf.p...|\n",
      "| 18|14981947918|https://smartstor...|pigg****|    그냥 저냥... 문이 두개다 보...|   2019-10-14|     0.800841|         4|['https://phinf.p...|\n",
      "| 19|14981947918|https://smartstor...|isiz****|    <em>캠프는 내취향이 아니야<...|   2021-07-04|     0.795836|         5|['https://phinf.p...|\n",
      "+---+-----------+--------------------+--------+----------------------------------+-------------+-------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 클렌징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규식으로 클렌징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|                            clean1|\n",
      "+----------------------------------+\n",
      "|원래 원터치텐트만 구입하려고 마...|\n",
      "|  좋아요 한여름에 창이  개가 아...|\n",
      "|        배송이 빨리왔어요      ...|\n",
      "|구매하기전에 정말 고민 많이했어...|\n",
      "|    첫 캠핑이라 너무 설레서    ...|\n",
      "|  캠핑 한번도 한적 없어요 설치 ...|\n",
      "|  추석 이후 올줄 알았는데 완전 ...|\n",
      "|이걸로 얼마전 태풍같이 바람불고...|\n",
      "| 같은날 주문한 품목들중 일부 품...|\n",
      "| 택배는 중간에 광복절 휴무가 끼...|\n",
      "|             너무 좋네요       ...|\n",
      "|  여름에 잘 썼습니다  그런데 육...|\n",
      "| 제가 사용 해보니 간편하고 튼튼...|\n",
      "|   일단 텐트가 아이둘 있는  인 ...|\n",
      "|여름이라 편하게 다니려고 원터치...|\n",
      "|간단하게 후기를 남기자면 텐트가...|\n",
      "|    아이들도 크고     날씨도 좋...|\n",
      "|남편이 체력거지 게으름뱅이 귀차...|\n",
      "|    그냥 저냥    문이 두개다 보...|\n",
      "|        캠프는 내취향이 아니야 ...|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#한글만 남기기\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "r = df.select(regexp_replace(col('review'), '[^\\uAC00-\\uD7A3]', ' ').alias('clean1'))\n",
    "\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|                            clean1|\n",
      "+----------------------------------+\n",
      "|원래 원터치텐트만 구입하려고 마...|\n",
      "|  좋아요 한여름에 창이  개가 아...|\n",
      "|        배송이 빨리왔어요      ...|\n",
      "|구매하기전에 정말 고민 많이했어...|\n",
      "|    첫 캠핑이라 너무 설레서    ...|\n",
      "|  캠핑 한번도 한적 없어요 설치 ...|\n",
      "|  추석 이후 올줄 알았는데 완전 ...|\n",
      "|이걸로 얼마전 태풍같이 바람불고...|\n",
      "| 같은날 주문한 품목들중 일부 품...|\n",
      "| 택배는 중간에 광복절 휴무가 끼...|\n",
      "|             너무 좋네요       ...|\n",
      "|  여름에 잘 썼습니다  그런데 육...|\n",
      "| 제가 사용 해보니 간편하고 튼튼...|\n",
      "|   일단 텐트가 아이둘 있는  인 ...|\n",
      "|여름이라 편하게 다니려고 원터치...|\n",
      "|간단하게 후기를 남기자면 텐트가...|\n",
      "|    아이들도 크고     날씨도 좋...|\n",
      "|남편이 체력거지 게으름뱅이 귀차...|\n",
      "|    그냥 저냥    문이 두개다 보...|\n",
      "|        캠프는 내취향이 아니야 ...|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 줄바꿈 제거\n",
    "\n",
    "r = r.select(regexp_replace(col('clean1'),'\\n', \"\").alias('clean1'))\n",
    "                            \n",
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이중 스페이스 제거\n",
    "\n",
    "r = r.select(regexp_replace(col('clean1'), '\\s+', ' ').alias('clean1'))\n",
    "                            \n",
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|                            clean1|\n",
      "+----------------------------------+\n",
      "|원래 원터치텐트만 구입하려고 마...|\n",
      "| 좋아요 한여름에 창이 개가 아니...|\n",
      "|  배송이 빨리왔어요 펼치고 접기...|\n",
      "|구매하기전에 정말 고민 많이했어...|\n",
      "| 첫 캠핑이라 너무 설레서 고민고...|\n",
      "|  캠핑 한번도 한적 없어요 설치 ...|\n",
      "|  추석 이후 올줄 알았는데 완전 ...|\n",
      "|이걸로 얼마전 태풍같이 바람불고...|\n",
      "| 같은날 주문한 품목들중 일부 품...|\n",
      "| 택배는 중간에 광복절 휴무가 끼...|\n",
      "|   너무 좋네요 하자없는지 택배 ...|\n",
      "| 여름에 잘 썼습니다 그런데 육각...|\n",
      "| 제가 사용 해보니 간편하고 튼튼...|\n",
      "|  일단 텐트가 아이둘 있는 인 가...|\n",
      "|여름이라 편하게 다니려고 원터치...|\n",
      "|간단하게 후기를 남기자면 텐트가...|\n",
      "| 아이들도 크고 날씨도 좋아져서 ...|\n",
      "|남편이 체력거지 게으름뱅이 귀차...|\n",
      "| 그냥 저냥 문이 두개다 보미확실...|\n",
      "|   캠프는 내취향이 아니야 라는 ...|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#한글만 남기기\n",
    "r = df.select(regexp_replace(col('review'), '[^\\uAC00-\\uD7A3]', ' ').alias('clean1'))\n",
    "\n",
    "# 줄바꿈 제거\n",
    "r = r.select(regexp_replace(col('clean1'),'\\n', \"\").alias('clean1'))\n",
    "\n",
    "# 이중 스페이스 제거\n",
    "r = r.select(regexp_replace(col('clean1'), '\\s+', ' ').alias('clean1'))\n",
    "\n",
    "r.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.write.text('./tent-review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o93.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1512)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:103)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:122)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:177)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:426)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:417)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:504)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:186)\n\t... 41 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-005dc254204b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bzip2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./tent-review.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o93.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1512)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:103)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:122)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:177)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:426)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:417)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:504)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:186)\n\t... 41 more\n"
     ]
    }
   ],
   "source": [
    "# r.write.mode(\"overwrite\").option(\"compression\",\"bzip2\").format(\"text\").save(\"./tent-review.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = r\"/home/ec2-user/notebook-work/miran/cleansing/tent-review/part-00000-ce1beab4-b9f2-4185-a763-d98fdfb3bbe2-c000.txt\"\n",
    "\n",
    "with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    review = f.readlines()\n",
    "\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveiw = review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum, Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['원래',\n",
       " '원',\n",
       " '터치',\n",
       " '텐트',\n",
       " '만',\n",
       " '구입',\n",
       " '하려고',\n",
       " '마음먹은',\n",
       " '사람',\n",
       " '입니다',\n",
       " '기존',\n",
       " '패스트',\n",
       " '캠프',\n",
       " '테라',\n",
       " '원',\n",
       " '터치',\n",
       " '텐트',\n",
       " '사용',\n",
       " '하다가',\n",
       " '좀',\n",
       " '작은',\n",
       " '사이즈',\n",
       " '도',\n",
       " '추가',\n",
       " '로',\n",
       " '구입',\n",
       " '하고',\n",
       " '싶어서',\n",
       " '오토',\n",
       " '로',\n",
       " '하나',\n",
       " '더',\n",
       " '구입',\n",
       " '했습니다',\n",
       " '손',\n",
       " '에',\n",
       " '익었을',\n",
       " '때',\n",
       " '펴는데',\n",
       " '분',\n",
       " '정도',\n",
       " '걸려요',\n",
       " '접는데도',\n",
       " '그',\n",
       " '정도',\n",
       " '는',\n",
       " '걸리는거',\n",
       " '같구요',\n",
       " '가방',\n",
       " '에',\n",
       " '넣었을',\n",
       " '때',\n",
       " '부피',\n",
       " '가',\n",
       " '작고',\n",
       " '가벼워서',\n",
       " '좋습니다',\n",
       " '미니',\n",
       " '멀',\n",
       " '캠핑',\n",
       " '이라',\n",
       " '구',\n",
       " '성품',\n",
       " '에',\n",
       " '있는',\n",
       " '번',\n",
       " '들',\n",
       " '팩',\n",
       " '으로도',\n",
       " '사용',\n",
       " '이',\n",
       " '충분하고',\n",
       " '추가',\n",
       " '팩',\n",
       " '구입',\n",
       " '할',\n",
       " '필요',\n",
       " '없을듯',\n",
       " '해',\n",
       " '요',\n",
       " '나무',\n",
       " '데크',\n",
       " '에서',\n",
       " '주로',\n",
       " '야영',\n",
       " '하니',\n",
       " '오징어',\n",
       " '팩',\n",
       " '만',\n",
       " '쓸',\n",
       " '때',\n",
       " '고',\n",
       " '있구요',\n",
       " '천고',\n",
       " '는',\n",
       " '테라',\n",
       " '가',\n",
       " '이어서',\n",
       " '엄청',\n",
       " '편했는데',\n",
       " '이번',\n",
       " '에',\n",
       " '산',\n",
       " '오토',\n",
       " '도',\n",
       " '그것',\n",
       " '보다는',\n",
       " '낮',\n",
       " '지만',\n",
       " '전혀',\n",
       " '불편',\n",
       " '감',\n",
       " '없네요',\n",
       " '레인',\n",
       " '플라이',\n",
       " '레인',\n",
       " '커버',\n",
       " '전용',\n",
       " '카페트',\n",
       " '그',\n",
       " '라운드',\n",
       " '시트',\n",
       " '익스',\n",
       " '텐션',\n",
       " '월',\n",
       " '추가',\n",
       " '구매',\n",
       " '했습니다',\n",
       " '타프',\n",
       " '가지',\n",
       " '고',\n",
       " '있는',\n",
       " '분',\n",
       " '은',\n",
       " '익스',\n",
       " '텐션',\n",
       " '월',\n",
       " '추가',\n",
       " '구매',\n",
       " '불필요할거',\n",
       " '같구요',\n",
       " '딱',\n",
       " '비',\n",
       " '피하',\n",
       " '고',\n",
       " '짐',\n",
       " '안',\n",
       " '젖을',\n",
       " '정도',\n",
       " '의',\n",
       " '공간',\n",
       " '만',\n",
       " '나와요',\n",
       " '미니',\n",
       " '멀',\n",
       " '캠핑',\n",
       " '하시는',\n",
       " '분',\n",
       " '은',\n",
       " '익스',\n",
       " '텐션',\n",
       " '월',\n",
       " '까지',\n",
       " '추가',\n",
       " '구입',\n",
       " '하시고',\n",
       " '타프',\n",
       " '사용',\n",
       " '안',\n",
       " '하시면',\n",
       " '될',\n",
       " '거',\n",
       " '같구요',\n",
       " '만',\n",
       " '원더',\n",
       " '보태',\n",
       " '서',\n",
       " '크기',\n",
       " '가',\n",
       " '좀더',\n",
       " '큰',\n",
       " '착탈',\n",
       " '익스',\n",
       " '텐션',\n",
       " '월',\n",
       " '사시는것도',\n",
       " '괜찮을거',\n",
       " '같구요',\n",
       " '텐트',\n",
       " '내부',\n",
       " '는',\n",
       " '어른',\n",
       " '명',\n",
       " '누우',\n",
       " '면',\n",
       " '딱',\n",
       " '적당할거',\n",
       " '같은',\n",
       " '사이즈',\n",
       " '인거',\n",
       " '같아요',\n",
       " '그리고',\n",
       " '원래',\n",
       " '쓰던',\n",
       " '테라',\n",
       " '는',\n",
       " '접어도',\n",
       " '부피',\n",
       " '가',\n",
       " '좀',\n",
       " '크고',\n",
       " '좀',\n",
       " '더',\n",
       " '무거웠고',\n",
       " '짐칸',\n",
       " '에',\n",
       " '충분히',\n",
       " '실리',\n",
       " '기는',\n",
       " '했습니다',\n",
       " '그',\n",
       " '에',\n",
       " '비해',\n",
       " '오토',\n",
       " '는',\n",
       " '부피',\n",
       " '도',\n",
       " '작고',\n",
       " '경차',\n",
       " '라도',\n",
       " '짐칸',\n",
       " '에',\n",
       " '언제',\n",
       " '든',\n",
       " '실어',\n",
       " '다닐수있어서',\n",
       " '좋습니다',\n",
       " '저',\n",
       " '같이',\n",
       " '귀차니즘',\n",
       " '에',\n",
       " '간편한거',\n",
       " '좋아하시면서',\n",
       " '미니',\n",
       " '멀',\n",
       " '캠핑',\n",
       " '하시는',\n",
       " '분',\n",
       " '들',\n",
       " '에게',\n",
       " '제격',\n",
       " '인',\n",
       " '텐트',\n",
       " '같아요',\n",
       " '한번',\n",
       " '설치',\n",
       " '해보면',\n",
       " '그',\n",
       " '담',\n",
       " '부터는',\n",
       " '아주',\n",
       " '쉽습니다',\n",
       " '저',\n",
       " '처럼',\n",
       " '캠핑',\n",
       " '초보',\n",
       " '나',\n",
       " '나들이',\n",
       " '용',\n",
       " '으로',\n",
       " '쓰실',\n",
       " '분',\n",
       " '들',\n",
       " '은',\n",
       " '저렴하게',\n",
       " '사서',\n",
       " '쓰시',\n",
       " '면',\n",
       " '좋을거',\n",
       " '같네요']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Okt()\n",
    "tokenizer.morphs(review[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from koalanlp import API\n",
    "from koalanlp.proc import Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-112-db412500eb88>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-112-db412500eb88>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from koalanlp.Util import initializeinitialize(hnn='LATEST')\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from koalanlp.Util import initializeinitialize(hnn='LATEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "사용 전 초기화 과정이 필요합니다. 사용법의 Util.initialize 문서를 참고하여 초기화를 해주세요.사용하신 코드를 토대로는 다음 코드의 실행을 추천해드립니다.\nfrom koalanlp.Util import initializeinitialize(hnn='LATEST')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-39d011b099c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSyntaxTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTreeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/koalanlp/proc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/koalanlp/proc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api, cls, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'etri_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0merror_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/koalanlp/API.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(api, type)\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0;34m\"사용하신 코드를 토대로는 다음 코드의 실행을 추천해드립니다.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0;34m\"from koalanlp.Util import initialize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         \"initialize(%s='LATEST')\" % api)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: 사용 전 초기화 과정이 필요합니다. 사용법의 Util.initialize 문서를 참고하여 초기화를 해주세요.사용하신 코드를 토대로는 다음 코드의 실행을 추천해드립니다.\nfrom koalanlp.Util import initializeinitialize(hnn='LATEST')"
     ]
    }
   ],
   "source": [
    "parser = Parser(API.HNN)\n",
    "parsed = parser(review[0])\n",
    "print(parsed[0].getSyntaxTree().getTreeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
